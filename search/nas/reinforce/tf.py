'''
 * @Author: romain.egele, dipendra.jha
 * @Date: 2018-06-20 14:59:13
'''
import os
import sys
import tensorflow as tf
import numpy as np
import random
import math

HERE = os.path.dirname(os.path.abspath(__file__)) # policy dir
top  = os.path.dirname(os.path.dirname(HERE)) # search dir
top  = os.path.dirname(os.path.dirname(top)) # dir containing deephyper
sys.path.append(top)

from deephyper.search.nas.policy.tf import NASCellPolicyV5
from deephyper.model.arch import StateSpace

class BasicReinforceV5:
    def __init__(self, sess, optimizer, policy_network, max_layers,
                batch_size,
                global_step,
                state_space=None,
                optimization_algorithm='PG',
                clip_param=0.3,
                entropy_param=0.0,
                vf_loss_param=0.0,
                division_rate=1.0,
                reg_param=0.001,
                discount_factor=0.99,
                exploration=0.5,
                exploration_decay = 0.1):
        '''
        Args
            sess: tensorflow session
            optimizer: optimizer used for controller
            policy_network: class of policy used by the controller
            max_layers: maximum number of layers generated by the controller
            batch_size: number of models generated by the controller
            global_step:
            state_space: search space definition used by the controller
            division_rate:
            reg_param:
            discount_factor:
            exploration:
        '''
        self.sess = sess
        self.exploration_ = exploration
        self.curr_step_exp = False
        self.optimizer = optimizer
        self.policy_network = policy_network
        self.division_rate = division_rate
        self.reg_param = reg_param
        self.discount_factor = discount_factor
        self.max_layers = max_layers
        self.batch_size = batch_size
        self.global_step = global_step
        self.state_space = state_space
        self.optimization_algorithm = optimization_algorithm
        self.clip_param = clip_param
        self.entropy_param = entropy_param
        self.vf_loss_param = vf_loss_param

        self.max_reward = -math.inf
        self.reward_list = [] # a reward is added only one time
        self.reward_buffer = [] # a reward is duplicate num_tokens time, corresponding to a 1D list with batch_size rewards num_tokens_in_one_batch time
        self.state_buffer = []
        self.rewards_b = 0
        self.pb_buffer = []

        self.create_variables()
        # self.policy_network.restore_model(sess = self.sess)
        init = tf.global_variables_initializer()
        sess.run(init)

    def get_actions(self, rnn_input, num_layers):
        '''
            Generations a list of index corresponding to the actions choosed by the controller.
            Args
                rnn_input: list of shape (batch_size) input of the rnn
                num_layers: int
        '''
        if self.state_space.feature_is_defined('skip_conn'):
            num_tokens_for_one = ((self.state_space.size - 1) * num_layers + num_layers * (num_layers - 1) // 2)
        else:
            num_tokens_for_one = self.state_space.size * num_layers
        self.num_tokens = num_tokens_for_one * self.batch_size

        policy_outputs_, so =  self.sess.run([
            self.policy_outputs[:self.num_tokens],
            self.so,
        ],
            {self.rnn_input: rnn_input,
             self.num_tokens_tensor: [self.num_tokens]})

        self.pb_buffer.append(so)
        print(f'shape so: {np.shape(so)}')

        return policy_outputs_, so

    def create_variables(self):
        self.rnn_input = tf.placeholder(
            dtype=tf.float32, shape=(self.batch_size), name='rnn_input')

        self.num_tokens_tensor = tf.placeholder(
            tf.int32, [1], name='num_tokens')

        self.batch_labels = tf.placeholder(
            tf.int32, [None, self.batch_size], name='labels')

        with tf.name_scope("predict_actions"):
            # initialize policy network
            with tf.variable_scope("policy_network"):
                # self.policy_outputs : tokens_inds_gather
                # self.before_softmax_outputs : softmax_inputs_tensor
                # self.so : softmax_out_prob
                self.policy_outputs, self.before_softmax_outputs, self.so = self.policy_network\
                    .get(self.rnn_input, self.max_layers)

            self.action_scores = tf.identity(
                self.policy_outputs, name="action_scores")

            self.predicted_action = tf.cast(
                tf.scalar_mul(
                    self.division_rate,
                    self.action_scores),
                    tf.float32,
                    name="predicted_action")

        # compute loss and gradients
        with tf.name_scope("compute_gradients"):
            # gradients for selecting action from policy network
            self.discounted_rewards = tf.placeholder(
                tf.float32,
                shape=(None),
                name="discounted_rewards")
            self.discounted_rewards = tf.Print(self.discounted_rewards,    [self.discounted_rewards], '#DISC REWARDS: ')


            self.pb_old = tf.placeholder(
                tf.float32,
                shape=[None, self.batch_size, self.policy_network.max_num_classes],
                name="pb_old"
            )

            self.before_softmax_outputs_slice = tf.slice(
                self.before_softmax_outputs,
                [0, 0, 0],
                [self.num_tokens_tensor[0] // self.batch_size,
                self.batch_size,
                self.policy_network.max_num_classes],
                name='before_softmax_outputs_slice')

            self.softmax_outputs_slice = tf.slice(
                self.so,
                [0, 0, 0],
                [self.num_tokens_tensor[0] // self.batch_size,
                self.batch_size,
                self.policy_network.max_num_classes],
                name='softmax_outputs_slice')

            # ENTROPY
            one_hot = tf.one_hot(self.batch_labels,
                depth=self.policy_network.max_num_classes,
                dtype=tf.float32)
            # print(f'one_hot.get_shape(): {one_hot.get_shape()}')
            one_hot = tf.transpose(one_hot, perm=[0, 2, 1])
            # one_hot = tf.Print(one_hot, [one_hot], '#one_hot: ')
            self.softmax_outputs_slice = tf.Print(self.softmax_outputs_slice, [self.softmax_outputs_slice], '#so slice: ', summarize=6)
            self.pb = tf.matmul(self.softmax_outputs_slice, one_hot)
            self.pb = tf.squeeze(self.pb)

            # ENTROPY == - self.entropy
            self.entropy = -self.entropy_param * tf.reduce_sum(tf.multiply(self.pb,
                                                                     tf.log(self.pb)))

            if self.optimization_algorithm == 'PG':
                self.cross_entropy_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(
                    logits=self.before_softmax_outputs_slice,
                    labels=self.batch_labels)

                self.cross_entropy_loss = tf.multiply(
                    self.cross_entropy_loss_,
                    self.discounted_rewards)

                self.pg_loss = tf.reduce_sum(self.cross_entropy_loss)
                self.pg_loss = tf.divide(self.pg_loss, self.batch_size)

                self.loss = self.pg_loss

                # Substract the ENTROPY from the LOSS, we are penalizing certainty
                self.loss = tf.subtract(self.loss, self.entropy)

            elif self.optimization_algorithm == 'PPO': # TODO
                pb_old = tf.matmul(self.pb_old, one_hot)
                pb_old = tf.squeeze(pb_old)
                atarg = self.discounted_rewards

                # ratio = tf.divide(self.pb, pb_old)
                ratio = tf.exp(self.pb - pb_old)
                # ratio = tf.Print(ratio, [ratio, self.softmax_outputs_slice, self.pb_old], '#ratio_1: ', summarize=10)
                # ratio = tf.Print(ratio, [ratio, self.pb, pb_old], '#ratio_2: ', summarize=10)
                # ratio = tf.reduce_sum(ratio)

                clip_obj = tf.multiply(atarg, tf.clip_by_value(ratio,
                                                 1.0 - self.clip_param,
                                                 1.0 + self.clip_param))
                # clip_obj = tf.Print(clip_obj, [clip_obj], '#clip_obj: ')

                unclip_obj = tf.multiply(atarg, ratio)
                # unclip_obj = tf.Print(unclip_obj, [unclip_obj], '#unclip_obj: ')

                # MAXIMUM only for benchmark function because loss < 0 -> minimum when loss > 0
                pol_surr = tf.cond(tf.less_equal(tf.norm(unclip_obj),
                                                 tf.norm(clip_obj)),
                                   lambda: unclip_obj,
                                   lambda: clip_obj)

                # pol_surr = tf.Print(pol_surr, [pol_surr], '#pol_surr tf.cond(...): ')
                pol_surr = tf.reduce_sum(pol_surr)
                # pol_surr = tf.Print(pol_surr, [pol_surr], '#pol_surr tf.reduce_mean(...): ')

                # self.loss = tf.add(-pol_surr, self.entropy)
                vf_loss = self.vf_loss_param * tf.reduce_mean(tf.square(self.discounted_rewards))
                # self.loss = tf.subtract(pol_surr, mse)
                # self.loss = tf.add(pol_surr, self.entropy)
                # self.loss= tf.subtract(self.loss, vf_loss)
                self.loss = -pol_surr
                # self.loss = pol_surr
                # tf.summary.scalar('pol_vf', mse)
                # tf.summary.scalar('pol_surr', pol_surr)
                tf.summary.scalar('loss', self.loss)

            # compute gradients
            self.gradients, var = zip(*self.optimizer.compute_gradients(self.loss))
            # for g, v in zip(self.gradients, var):
            #     if g is not None:
            #         tf.summary.histogram(v.name, v)
            #         tf.summary.histogram(v.name + '_grad', g)

            # self.gradients = [
                # None if gradient is None else tf.clip_by_norm(gradient, 5.0)
                # for gradient in self.gradients]

            # training update
            with tf.name_scope("train_policy_network"):
                # apply gradients to update policy network
                # self.train_op = self.optimizer.minimize(self.loss)
                # Exploding gradients leads to NaN in dynamic RNN
                # self.gradients, _ = tf.clip_by_global_norm(self.gradients, 1.0)
                # self.gradients = tf.Print(self.gradients, [self.gradients], "#clip gradients: ", summarize=6)
                self.train_op = self.optimizer.apply_gradients(
                    zip(self.gradients, var), global_step=self.global_step)

            # for v in tf.trainable_variables():
            #     tf.summary.histogram(v.name, v)
            # self.merged = tf.summary.merge_all()
            # self.writer = tf.summary.FileWriter('train_log_layer', tf.get_default_graph())


    def storeRollout(self, state, rewards, num_layers):
        '''
        Args
            state:
            rewards: a list
            num_layers: an int
        '''
        if self.state_space.feature_is_defined('skip_conn'):
            num_tokens_for_one = ((self.state_space.size - 1) * num_layers + num_layers * (num_layers - 1) // 2)
        else:
            num_tokens_for_one = self.state_space.size * num_layers
        self.num_tokens = num_tokens_for_one * self.batch_size

        tmp_max = max(rewards)
        if self.max_reward < tmp_max:
            self.max_reward = tmp_max
            self.reward_list.append(self.max_reward)

        for i in range(0, self.num_tokens, self.batch_size):
            # discount
            rewards = [(self.discount_factor**i) * x for j, x in enumerate(rewards)]
            self.reward_buffer.extend(rewards)
            self.state_buffer.extend(state[-i-self.batch_size:][:self.batch_size])

    def train_step(self, num_layers, init_state, global_step):
        if self.optimization_algorithm == 'PG':
            return self.train_step_PG(num_layers, init_state)
        else:
            return self.train_step_PPO(num_layers, init_state, global_step)

    def train_step_PG(self, num_layers, init_state):
        '''
        Policy Gradient Optimization
        '''
        if self.state_space.feature_is_defined('skip_conn'):
            num_tokens_for_one = ((self.state_space.size - 1) * num_layers + num_layers * (num_layers - 1) // 2)
        else:
            num_tokens_for_one = self.state_space.size * num_layers
        self.num_tokens = num_tokens_for_one * self.batch_size

        steps_count = self.num_tokens
        states = np.reshape(np.array(self.state_buffer[-steps_count:]),
        (self.num_tokens // self.batch_size, self.batch_size)) / self.division_rate
        prev_state = init_state
        rewards = self.reward_buffer[-steps_count:]
        self.rewards_b = ema(self.reward_list[:-1], 0.9, self.rewards_b, self.batch_size)

        self.R_b = [(x - self.rewards_b) for x in rewards]
        self.R_b = np.reshape(np.array(self.R_b), (self.num_tokens//self.batch_size, self.batch_size))
        _, ls = self.sess.run([self.train_op,
                               self.loss],
                                   {self.rnn_input: prev_state,
                                    self.discounted_rewards: self.R_b,
                                    self.batch_labels: states,
                                    self.num_tokens_tensor: [self.num_tokens]})
        # self.policy_network.save_model(self.sess)
        return ls

    def train_step_PPO(self, num_layers, init_state, global_step):
        '''
        Proximal Policy Gradient Optimization
        '''
        if self.state_space.feature_is_defined('skip_conn'):
            num_tokens_for_one = ((self.state_space.size - 1) * num_layers + num_layers * (num_layers - 1) // 2)
        else:
            num_tokens_for_one = self.state_space.size * num_layers
        self.num_tokens = num_tokens_for_one * self.batch_size

        rewards = self.reward_buffer[-self.num_tokens:]
        self.rewards_b = ema(self.reward_list[:-1], 0.9, self.rewards_b, self.batch_size)

        self.R_b = [(x - self.rewards_b) for x in rewards]
        self.R_b = np.reshape(np.array(self.R_b), (self.num_tokens//self.batch_size, self.batch_size))

        states = np.reshape(np.array(self.state_buffer[-self.num_tokens:]),
            (self.num_tokens // self.batch_size, self.batch_size)) / self.division_rate
        prev_state = init_state
        if len(self.pb_buffer) >= 2:
            pb_old = self.pb_buffer[-2]
        else:
            pb_old = self.pb_buffer[-1]

        _, ls, = self.sess.run([self.train_op,
                                        self.loss],
                               {
                                   self.rnn_input: prev_state,
                                   self.discounted_rewards: self.R_b,
                                   self.batch_labels: states,
                                   self.num_tokens_tensor: [self.num_tokens],
                                   self.pb_old: pb_old
                               })
        # self.writer.add_summary(summary, global_step)
        # self.writer.flush()

def sma(data, window):
    """
    Calculates Simple Moving Average
    http://fxtrade.oanda.com/learn/forex-indicators/simple-moving-average
    """
    if len(data) < window:
        return None
    return sum(data[-window:]) / float(window)

def ema_old(data, window):
    if len(data) < 2 * window:
        #return sum(data)/len(data)
        return data[-1]
    c = 2.0 / (window + 1)
    current_ema = sma(data[-window*2:-window], window)
    for value in data[-window:]:
        current_ema = (c * value) + ((1 - c) * current_ema)
    return current_ema

def ema(reward_list, alpha, prev_sum, window=1):
    if len(reward_list) < window:
        return 0
    elif True:
        return max(reward_list)
    elif len(reward_list) == window:
        return sum(reward_list[:])/window
    else:
        return alpha * sum(reward_list[-window:])/window + (1-alpha)*prev_sum


def test_BasicReinforce5():
    session = tf.Session()
    global_step = tf.Variable(0, trainable=False)
    state_space = StateSpace()
    state_space.add_state('filter_size', [10., 20., 30.])
    state_space.add_state('num_filters', [32., 64.])
    state_space.add_state('skip_conn', [])
    policy_network = NASCellPolicyV5(state_space, save_path='savepoint/model')
    max_layers = 4
    batch_size = 1

    learning_rate = tf.train.exponential_decay(0.99, global_step,
                                                500, 0.96, staircase=True)

    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)

    # for the CONTROLLER
    reinforce = BasicReinforceV5(session,
                                optimizer,
                                policy_network,
                                max_layers,
                                batch_size,
                                global_step,
                                state_space=state_space)
    init_seeds = [1. for i in range(batch_size)]
    for num_layers in range(1, max_layers):
        actions, _ = reinforce.get_actions(init_seeds, num_layers)
        rewards = [.90] * batch_size
        print(f' num_layer: {num_layers} action = {actions} rewards = {rewards}')
        reinforce.storeRollout(actions, rewards, num_layers)
        print(f'num_layers: {num_layers}, init_seeds: {init_seeds}')
        reinforce.train_step(num_layers, init_seeds, 1)


if __name__ == '__main__':
    test_BasicReinforce5()
